# Spotify Playlist Data Analysis | ML

Attempting to predict song recommendations for a given playlist - similar to concept behind Spotify's 'Smart Shuffle' feature. 

Stages:

## Data Source
Since this project aims to predict whether a song would fit into a playlist of pre-existing songs – and there are lots of pre-existing songs – we require a large amount of data to perform our analyses. Specifically, we need to work with a large collection of playlists containing a variety of different songs. In our data acquisition step, we found an existing data challenge site [here](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge) that included data that met these requirements. This dataset was meant for people trying to do a similar Spotify playlist prediction task as ours, thus proving its usefulness to our cause. In addition to this data, however, we also wanted to add Spotify’s “audio features” for all the songs in these playlists. 
We were able to combine these datasets (the AICrowd data and the Spotify audio features data) by using Spotify’s Web API. In our data preparation notebook, we made API calls for every unique song in our database, and matched the results with the playlist data. Our process of data acquisition is outlined in more detail below. Through this preparation, we built a dataset that had both playlist–song relationships as well as song–attribute relationships that we could use going forward. Moving forward, we will use the various audio feature fields as explanatory variables. These include attributes such as danceability, energy, and loudness, which differ from song to song. We will attempt to observe the effect of these on the playlist ID. As our target variable, playlist ID refers to which playlist a song is part of. Understanding this relationship between song and playlist based on the attributes of songs in the playlist is key to this project. 

## Data Collection and EDA 
The first step in our EDA was data retrieval. The initial dataset was stored as a folder of JSON files which needed to be read in and concatenated into a dataframe. After formatting this dataset, we spent time building a solution to pull audio data from Spotify’s Web API and combine it with the existing dataset. Within this script, the API calls would return a dictionary of JSON files that needed to be parsed and formatted into a separate dataframe. Now having a dataframe of playlists and songs as well as a separate DataFrame for songs and their attributes, we joined the two dataframes on the song ID to create a dataset that now holds playlists, songs in the playlists, and those songs’ attributes. 
Once we had collected all the data, we needed to clean it before looking at any trends. We did this in three stages. First, we removed all extraneous columns. Track ID, for instance, is no longer of interest to us since we’ve already merged the data. This and other columns were dropped for readability. Second, we corrected the types of a few attributes. Mode and key were both encoded in the data as numeric values, but are actually much better represented as categorical variables. To fix this, we simply wrote code to translate the numeric values to their categorical counterparts, thus alleviating the issue. Finally, we removed any missing data. We found “missing” data to be in two forms. Some data was obviously missing (i.e. populated with NaN values) while other data was more subtle. We found some songs that had values for all the attributes, but these values were all set to 0 (or -60 in the case of loudness). While these are technically realistic values, we checked a few of these songs, and it is much more realistic that these 0s are placeholders for missing data. As a result, we removed all songs that had these placeholder 0s or contained NaN values. Furthermore, we removed all playlists that contained such songs, since it did not make sense to analyze a playlist without all of its original songs. After doing this, we were left with 9977 of our original 10000 playlists. 
The final step of EDA we performed looked at potential trends in the data. We did this in two parts. First we looked at trends across all songs in our database to understand the broad distribution of attributes. For instance, we looked at whether the danceability of a song is related at all to the energy of a song. While the resulting scatter plot looked like there might be a correlation, we found there to be only a correlation coefficient of 0.09, which indicates that there is not a strong relationship there. We also looked at the relationship between loudness and valence, and found a stronger, but still weak correlation of 0.22. After this, we instead turned our focus to playlists as a whole. To do this, we grouped the songs by playlist and averaged the attributes for all the songs in each playlist and conducted similar exploratory visualizations. One trend that we noticed was that, on the playlist level, acousticness and energy appeared to form a somewhat-linear relationship. Upon further testing, we discovered that the two variables had a correlation coefficient of -0.88, which suggests a strong negative correlation. In other words, as a playlist gets more acoustic, it gets less energetic, which logically makes sense. These first few visualizations helped us get a better idea of the data we are working with and will help motivate or direct our approach later in our project. 

## Model Training
Algorithms to be Explored: 
Support Vector Machines (SVM): Given its prowess in managing high-dimensional spaces, SVM is an ideal candidate for our feature-rich dataset. It is renowned for its classification accuracy and is less prone to overfitting. Parameters like the regularization constant C, kernel type, and gamma will be fine-tuned to enhance its performance further. Random Forest: This ensemble method is not just robust but also adept at handling non-linear data. Random Forest will provide insights into feature importance, allowing us to understand what drives playlist suitability. We will optimize the number of trees and the maximum number of features to improve its predictive power. Gradient Boosting Machines (GBM): GBM stands out for its iterative refinement process, building models that learn from the errors of their predecessors. Its strength lies in its ability to capture complex patterns that other models may miss. We will be tuning the number of boosting stages, the depth of trees, and the learning rate to find the sweet spot that balances bias and variance.

Model Training and Evaluation Plan:
We will adopt a stratified split approach for our dataset, allocating 80% to training and 20% to validation to maintain the distribution of the classes. We aim to implement a rigorous cross-validation framework to test our models' robustness across different subsets of data. Our evaluation will be multi-faceted, encompassing accuracy, precision, recall, and the F1 score, which is particularly important given the multi-class nature of our target variable. This comprehensive evaluation will ensure that our model performs consistently and is not biased towards any particular class.

Hyper-parameter Tuning Strategy:
For SVM, we will explore various C values, kernels (linear, polynomial, RBF), and gamma settings. Random Forest's tuning will involve experimenting with different counts of estimators and the maximum number of features. For GBM, we will meticulously adjust the learning rate, the number of stages for boosting, and tree complexity parameters such as tree depth.
This systematic and exhaustive search will be instrumental in identifying the optimal combination of parameters that yield the best performance.


Accuracy and Metric Justification: 
Accuracy alone can be misleading, especially in multi-class settings. Therefore, we propose to use a confusion matrix and the F1 score as our primary metrics. The F1 score, a harmonic mean of precision and recall, is particularly important for our application as it balances the proportion of correctly identified positive cases against the importance of not missing a potential hit song for a playlist. Additionally, these metrics allow us to effectively navigate and trade-off between type I and II errors, which is crucial for maintaining the integrity and user satisfaction with the automated playlist selections.
